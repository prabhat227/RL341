{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b95c9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Value Function:\n",
      "[[ 0.10036594  0.23751681  0.32874565 -0.57367431]\n",
      " [ 0.07561819  0.         -0.67558146  0.        ]\n",
      " [ 0.06217491  0.03735872 -0.05738241 -0.16604593]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld size and initialize the value function\n",
    "rows, cols = 3, 4\n",
    "V = np.zeros((rows, cols))\n",
    "\n",
    "# Define the policy (directions)\n",
    "# Actions: 0 = Up, 1 = Down, 2 = Left, 3 = Right\n",
    "policy = [\n",
    "    [0, 3, 3, 1],\n",
    "    [0, None, 3, None],\n",
    "    [0, 2, 2, 2]\n",
    "]\n",
    "\n",
    "# Define the rewards for each state\n",
    "rewards = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, None, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Define discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the convergence threshold\n",
    "theta = 0.0001\n",
    "\n",
    "# Policy evaluation\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy[i][j] is not None:\n",
    "                v = V[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V[next_i][next_j])\n",
    "                V[i][j] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Print the final value function\n",
    "print(\"Final Value Function:\")\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5159954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Value Function for Policy 1:\n",
      "[[0.52563271 0.6274941  0.8312906  0.        ]\n",
      " [0.39671355 0.         0.         0.        ]\n",
      " [0.33185641 0.24672673 0.17179093 0.01002189]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld size and initialize the value function\n",
    "rows, cols = 3, 4\n",
    "V_policy1 = np.zeros((rows, cols))\n",
    "V_policy2 = np.zeros((rows, cols))\n",
    "\n",
    "# Define the policy (directions)\n",
    "# Actions: 0 = Up, 1 = Down, 2 = Left, 3 = Right\n",
    "policy1 = [\n",
    "    [3, 3, 3, 1],\n",
    "    [0, None, 3, None],\n",
    "    [0, 2, 2, 2]\n",
    "]\n",
    "\n",
    "policy2 = [\n",
    "    [1, 2, 3, 1],\n",
    "    [1, None, 3, None],\n",
    "    [3, 1, 2, 2]\n",
    "]\n",
    "\n",
    "# Define the rewards for each state\n",
    "rewards = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, None, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Define discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the convergence threshold\n",
    "theta = 0.0001\n",
    "\n",
    "# Policy evaluation for Policy 1\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy1[i][j] is not None:\n",
    "                v = V_policy1[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy1[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V_policy1[next_i][next_j])\n",
    "                V_policy1[i][j] = max(new_v, 0)  # Ensure values are non-negative\n",
    "                delta = max(delta, abs(v - V_policy1[i][j]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Final Value Function for Policy 1:\")\n",
    "print(V_policy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7876199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Value Function for Policy 1:\n",
      "[[0.52563271 0.6274941  0.8312906  0.        ]\n",
      " [0.39671355 0.         0.         0.        ]\n",
      " [0.33185641 0.24672673 0.17179093 0.01002189]]\n",
      "\n",
      "Final Value Function for Policy 2:\n",
      "[[1.00028355e-02 8.38175138e-02 7.77519866e-01 0.00000000e+00]\n",
      " [1.08007195e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.38694691e-04 3.17477470e-05 2.17072514e-05 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Policy evaluation for Policy 2\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy2[i][j] is not None:\n",
    "                v = V_policy2[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy2[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V_policy2[next_i][next_j])\n",
    "                V_policy2[i][j] = max(new_v, 0)  # Ensure values are non-negative\n",
    "                delta = max(delta, abs(v - V_policy2[i][j]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Print the final value functions for both policies\n",
    "print(\"Final Value Function for Policy 1:\")\n",
    "print(V_policy1)\n",
    "print(\"\\nFinal Value Function for Policy 2:\")\n",
    "print(V_policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecec32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Value Function for Policy 1:\n",
      "['0.25', '0.60', '0.83', '0.00']\n",
      "['0.19', '0.00', '0.00', '0.00']\n",
      "['0.16', '0.12', '0.08', '0.00']\n",
      "\n",
      "Final Value Function for Policy 2:\n",
      "['0.05', '0.11', '0.78', '0.00']\n",
      "['0.03', '0.00', '0.00', '0.00']\n",
      "['0.03', '0.01', '0.01', '0.00']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld size and initialize the value function\n",
    "rows, cols = 3, 4\n",
    "V_policy1 = np.zeros((rows, cols))\n",
    "V_policy2 = np.zeros((rows, cols))\n",
    "\n",
    "# Define the policy (directions)\n",
    "# Actions: 0 = Up, 1 = Down, 2 = Left, 3 = Right\n",
    "policy1 = [\n",
    "    [0, 3, 3, 1],\n",
    "    [0, None, 3, None],\n",
    "    [0, 2, 2, 2]\n",
    "]\n",
    "\n",
    "policy2 = [\n",
    "    [0, 2, 3, 1],\n",
    "    [0, None, 3, None],\n",
    "    [0, 1, 2, 2]\n",
    "]\n",
    "\n",
    "# Define the rewards for each state\n",
    "rewards = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, None, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Define discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the convergence threshold\n",
    "theta = 0.0001\n",
    "\n",
    "# Policy evaluation for Policy 1\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy1[i][j] is not None:\n",
    "                v = V_policy1[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy1[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V_policy1[next_i][next_j])\n",
    "                V_policy1[i][j] = max(new_v, 0)  # Ensure values are non-negative\n",
    "                delta = max(delta, abs(v - V_policy1[i][j]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Policy evaluation for Policy 2\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy2[i][j] is not None:\n",
    "                v = V_policy2[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy2[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V_policy2[next_i][next_j])\n",
    "                V_policy2[i][j] = max(new_v, 0)  # Ensure values are non-negative\n",
    "                delta = max(delta, abs(v - V_policy2[i][j]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Print the final value functions for both policies in grid format\n",
    "print(\"Final Value Function for Policy 1:\")\n",
    "for row in V_policy1:\n",
    "    print([\"{:.2f}\".format(val) for val in row])\n",
    "\n",
    "print(\"\\nFinal Value Function for Policy 2:\")\n",
    "for row in V_policy2:\n",
    "    print([\"{:.2f}\".format(val) for val in row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca76df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Value Function for Policy 1:\n",
      "0.49\t0.62\t0.83\t0.00\t\n",
      "0.06\t0.00\t0.00\t0.00\t\n",
      "0.01\t0.01\t0.00\t0.00\t\n",
      "\n",
      "Final Value Function for Policy 2:\n",
      "0.00\t0.01\t0.12\t0.00\t\n",
      "0.00\t0.00\t0.00\t0.00\t\n",
      "0.00\t0.00\t0.00\t0.00\t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld size and initialize the value function\n",
    "rows, cols = 3, 4\n",
    "V_policy1 = np.zeros((rows, cols))\n",
    "V_policy2 = np.zeros((rows, cols))\n",
    "\n",
    "# Define the policy (directions)\n",
    "# Actions: 0 = Up, 1 = Down, 2 = Left, 3 = Right\n",
    "policy1 = [\n",
    "    [3, 3, 3, None],\n",
    "    [1, None, 3, None],\n",
    "    [3, 2, 0, 2]\n",
    "]\n",
    "\n",
    "policy2 = [\n",
    "    [1, 2, 2, None],\n",
    "    [0, None, 1, None],\n",
    "    [0, 1, 2, 0]\n",
    "]\n",
    "\n",
    "# Define the rewards for each state\n",
    "rewards = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, None, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Define discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the convergence threshold\n",
    "theta = 0.0001\n",
    "\n",
    "# Policy evaluation for Policy 1\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy1[i][j] is not None:\n",
    "                v = V_policy1[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy1[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V_policy1[next_i][next_j])\n",
    "                V_policy1[i][j] = max(new_v, 0)  # Ensure values are non-negative\n",
    "                delta = max(delta, abs(v - V_policy1[i][j]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Policy evaluation for Policy 2\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if policy2[i][j] is not None:\n",
    "                v = V_policy2[i][j]\n",
    "                new_v = 0\n",
    "                for action in range(4):\n",
    "                    if action == policy2[i][j]:\n",
    "                        prob = 0.7\n",
    "                    else:\n",
    "                        prob = 0.1\n",
    "                    next_i, next_j = i, j\n",
    "                    if action == 0:  # Up\n",
    "                        next_i = max(i - 1, 0)\n",
    "                    elif action == 1:  # Down\n",
    "                        next_i = min(i + 1, rows - 1)\n",
    "                    elif action == 2:  # Left\n",
    "                        next_j = max(j - 1, 0)\n",
    "                    elif action == 3:  # Right\n",
    "                        next_j = min(j + 1, cols - 1)\n",
    "                    if rewards[next_i][next_j] is not None:\n",
    "                        new_v += prob * (rewards[next_i][next_j] + gamma * V_policy2[next_i][next_j])\n",
    "                V_policy2[i][j] = max(new_v, 0)  # Ensure values are non-negative\n",
    "                delta = max(delta, abs(v - V_policy2[i][j]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Print the final value functions for both policies in a closed rectangular grid format\n",
    "print(\"Final Value Function for Policy 1:\")\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        print(\"{:.2f}\".format(V_policy1[i][j]), end=\"\\t\")\n",
    "    print()  # Move to the next row\n",
    "\n",
    "print(\"\\nFinal Value Function for Policy 2:\")\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        print(\"{:.2f}\".format(V_policy2[i][j]), end=\"\\t\")\n",
    "    print()  # Move to the next row\n",
    "------------+*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61250f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
